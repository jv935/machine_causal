{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf18fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from load_data import (\n",
    "    PdMConfig, \n",
    "    load_pdm_raw, \n",
    "    build_hourly_panel, \n",
    "    infer_data_types, \n",
    "    scale_continuous_columns, \n",
    "    panel_to_machine_datasets\n",
    ")\n",
    "from pcmci_mixed import pcmciplus_mixed\n",
    "from graph_io import (\n",
    "    aggregate_graphs, \n",
    "    generalgraph_to_dict, \n",
    "    save_graph_json,\n",
    "    aggregate_info_asdict,\n",
    ")\n",
    "\n",
    "from nbcb_w import NBCBw\n",
    "from nbcb_e import NBCBe\n",
    "from cbnb_w import CBNBw\n",
    "from cbnb_e import CBNBe\n",
    "\n",
    "from causallearn.graph.Edge import Edge\n",
    "from causallearn.graph.Endpoint import Endpoint\n",
    "from causallearn.graph.GeneralGraph import GeneralGraph\n",
    "from causallearn.graph.GraphNode import GraphNode\n",
    "\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959184e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TELEMETRY_COLS = [\"volt\", \"rotate\", \"pressure\", \"vibration\"]\n",
    "HYBRID_ALGORITHMS_AVAILABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badce428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def granger_lasso_graph(\n",
    "    data: pd.DataFrame, \n",
    "    *, \n",
    "    tau_max: int = 5, \n",
    "    sig_level: float = 0.05, \n",
    "    cv: int = 5\n",
    ") -> GeneralGraph:\n",
    "    \"\"\"\n",
    "    Re-implements algorithms.py::granger_lasso but without extra dependencies.\n",
    "    (GCMVL in the IT paper codebase.)\n",
    "\n",
    "    Important:\n",
    "    - Uses LassoCV coefficients; then thresholds by |coef| > sig_level (as in the provided code).\n",
    "    - Collapses all lags into a single summary graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Time series data (T x N)\n",
    "    tau_max : int\n",
    "        Maximum lag to consider (must be >= 1)\n",
    "    sig_level : float\n",
    "        Coefficient threshold for edge inclusion\n",
    "    cv : int\n",
    "        Cross-validation folds for LassoCV\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    GeneralGraph\n",
    "        Summary causal graph\n",
    "    \"\"\"\n",
    "    if tau_max < 1:\n",
    "        raise ValueError(\"tau_max must be >= 1 for granger_lasso_graph\")\n",
    "\n",
    "    data = data.copy()\n",
    "    data = data.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    n, dim = data.shape\n",
    "    if n <= tau_max + 1:\n",
    "        raise ValueError(f\"Not enough samples (n={n}) for tau_max={tau_max}\")\n",
    "\n",
    "    # Stack lagged features: [X_{t-1}, X_{t-2}, ..., X_{t-tau_max}]\n",
    "    Y = data.values[tau_max:]\n",
    "    X = np.hstack([data.values[tau_max - k:-k] for k in range(1, tau_max + 1)])\n",
    "\n",
    "    lasso_cv = LassoCV(cv=cv)\n",
    "    coeff = np.zeros((dim, dim * tau_max))\n",
    "    for i in range(dim):\n",
    "        lasso_cv.fit(X, Y[:, i])\n",
    "        coeff[i] = lasso_cv.coef_\n",
    "\n",
    "    names = list(data.columns)\n",
    "    # dataset[target][source] = 2 means source -> target (same convention as algorithms.py)\n",
    "    dataset = pd.DataFrame(np.zeros((dim, dim), dtype=int), columns=names, index=names)\n",
    "\n",
    "    for i in range(dim):          # target index\n",
    "        for lag in range(tau_max):  # lag index\n",
    "            for j in range(dim):  # source index\n",
    "                if abs(coeff[i, j + lag * dim]) > sig_level:\n",
    "                    dataset.loc[names[j], names[i]] = 2\n",
    "\n",
    "    nodes = [GraphNode(n) for n in names]\n",
    "    node_map = {n.get_name(): n for n in nodes}\n",
    "    g = GeneralGraph(nodes)\n",
    "\n",
    "    for src in names:\n",
    "        for dst in names:\n",
    "            if src == dst:\n",
    "                continue\n",
    "            src_to_dst = dataset.loc[src, dst] == 2\n",
    "            dst_to_src = dataset.loc[dst, src] == 2\n",
    "            if src_to_dst and dst_to_src:\n",
    "                # Bidirectional \n",
    "                g.add_edge(Edge(node_map[src], node_map[dst], Endpoint.ARROW, Endpoint.ARROW))\n",
    "            elif src_to_dst:\n",
    "                g.add_edge(Edge(node_map[src], node_map[dst], Endpoint.TAIL, Endpoint.ARROW))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56e8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def varlingam_graph(\n",
    "    data: pd.DataFrame, \n",
    "    *, \n",
    "    tau_max: int = 1, \n",
    "    min_abs_effect: float = 0.0\n",
    ") -> GeneralGraph:\n",
    "    \"\"\"\n",
    "    VARLiNGAM summary graph.\n",
    "\n",
    "    Notes:\n",
    "    - Requires `lingam` (PyPI) to be installed.\n",
    "    - We add a directed edge src -> dst if ANY lagged adjacency matrix has a nonzero (or >min_abs_effect) effect.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Time series data (T x N)\n",
    "    tau_max : int\n",
    "        Number of lags for VAR model\n",
    "    min_abs_effect : float\n",
    "        Minimum absolute effect size for edge inclusion\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    GeneralGraph\n",
    "        Summary causal graph\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from lingam import VARLiNGAM\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"VARLiNGAM requires the `lingam` package. Install with: pip install lingam\"\n",
    "        ) from e\n",
    "\n",
    "    data = data.copy()\n",
    "    data = data.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    model = VARLiNGAM(lags=tau_max, criterion=\"bic\", prune=True)\n",
    "    model.fit(data)\n",
    "\n",
    "    # In VARLiNGAM, adjacency matrices are usually list/array of shape (lags, dim, dim)\n",
    "    mats = getattr(model, \"_adjacency_matrices\", None)\n",
    "    if mats is None:\n",
    "        raise RuntimeError(\"Could not access VARLiNGAM adjacency matrices (model._adjacency_matrices).\")\n",
    "\n",
    "    mats_arr = np.asarray(mats)\n",
    "    if mats_arr.ndim == 2:\n",
    "        # (dim, dim) single matrix\n",
    "        mats_arr = mats_arr[None, :, :]\n",
    "\n",
    "    names = list(data.columns)\n",
    "    node_map = {n: GraphNode(n) for n in names}\n",
    "    g = GeneralGraph(list(node_map.values()))\n",
    "\n",
    "    # Track edges to avoid duplicates\n",
    "    added_edges = set()\n",
    "    \n",
    "    # Convention: mats[l][i,j] is effect from j -> i (source j causes target i)\n",
    "    for B in mats_arr:\n",
    "        dim = B.shape[0]\n",
    "        for i in range(dim):      # target\n",
    "            for j in range(dim):  # source\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if abs(B[i, j]) > min_abs_effect:\n",
    "                    edge_key = (names[j], names[i])\n",
    "                    if edge_key not in added_edges:\n",
    "                        g.add_edge(Edge(node_map[names[j]], node_map[names[i]], Endpoint.TAIL, Endpoint.ARROW))\n",
    "                        added_edges.add(edge_key)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb05700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_machines_by_model(\n",
    "    machines: pd.DataFrame, \n",
    "    *, \n",
    "    n: int, \n",
    "    seed: int\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Select n machines per model type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    machines : pd.DataFrame\n",
    "        PdM_machines.csv as a DataFrame with columns 'machineID' and 'model'\n",
    "    n : int\n",
    "        Number of machines to select per model\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[int]]\n",
    "        Dictionary mapping model type to list of machine IDs\n",
    "    \"\"\"\n",
    "    if \"model\" not in machines.columns or \"machineID\" not in machines.columns:\n",
    "        raise ValueError(\"machines dataframe must include columns: 'model', 'machineID'\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    out: Dict[str, List[int]] = {}\n",
    "    for model in sorted(machines[\"model\"].unique().tolist()):\n",
    "        ids = machines.loc[machines[\"model\"] == model, \"machineID\"].astype(int).sort_values().tolist()\n",
    "        if len(ids) <= n:\n",
    "            out[model] = ids\n",
    "        else:\n",
    "            out[model] = rng.choice(ids, size=n, replace=False).astype(int).tolist()\n",
    "    return out\n",
    "\n",
    "\n",
    "def _prep_df(df: pd.DataFrame, *, max_rows: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"Prepare dataframe: handle inf/nan, optionally truncate, drop constant columns.\"\"\"\n",
    "    df = df.sort_index().copy()\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    if max_rows is not None and len(df) > max_rows:\n",
    "        df = df.iloc[:max_rows].copy()\n",
    "        \n",
    "    # Drop constant columns (some algorithms don't like them)\n",
    "    nunique = df.nunique(dropna=False)\n",
    "    keep = nunique[nunique > 1].index.tolist()\n",
    "    return df[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_per_machine(\n",
    "    algo_name: str,\n",
    "    algo_fn,\n",
    "    datasets: Dict[int, pd.DataFrame],\n",
    "    *,\n",
    "    min_support: int = 1,\n",
    ") -> Tuple[Dict[int, Any], Any, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run a (DataFrame -> GeneralGraph) algorithm on each machine, then aggregate by edge-support.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[int, Any], Any, Dict[str, Any]]\n",
    "        (per_machine_graphdict, aggregate_graphdict, aggregate_meta)\n",
    "    \"\"\"\n",
    "    per_machine: Dict[int, Any] = {}\n",
    "    graphs = []\n",
    "    errors: Dict[int, str] = {}\n",
    "    \n",
    "    for mid, df in datasets.items():\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            g = algo_fn(df)\n",
    "            per_machine[mid] = generalgraph_to_dict(g, metadata={\"runtime_s\": time.time() - t0})\n",
    "            graphs.append(g)\n",
    "        except Exception as e:\n",
    "            errors[mid] = f\"{type(e).__name__}: {e}\"\n",
    "            per_machine[mid] = None\n",
    "\n",
    "    agg_graph = None\n",
    "    agg_meta: Dict[str, Any] = {\"min_support\": min_support, \"errors\": errors}\n",
    "    \n",
    "    if len(graphs) > 0:\n",
    "        g_agg, info = aggregate_graphs(graphs, min_support=min_support)\n",
    "        agg_meta[\"n_graphs_ok\"] = info.n_graphs\n",
    "        agg_meta[\"edge_support\"] = {str(k): int(v) for k, v in info.edge_support.items()}\n",
    "        agg_graph = generalgraph_to_dict(g_agg, metadata={\"aggregation\": aggregate_info_asdict(info)})\n",
    "    else:\n",
    "        agg_meta[\"n_graphs_ok\"] = 0\n",
    "\n",
    "    return per_machine, agg_graph, agg_meta\n",
    "\n",
    "\n",
    "def _write_result(path: Path, result: Dict[str, Any]) -> None:\n",
    "    \"\"\"Write result dictionary to JSON file.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbbc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_model(\n",
    "    *,\n",
    "    data_dir: Path,\n",
    "    out_dir: Path,\n",
    "    model: str,\n",
    "    machine_ids: List[int],\n",
    "    tau_max: int,\n",
    "    sig_level: float,\n",
    "    max_rows_per_machine: Optional[int],\n",
    "    min_support: int,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Worker for one model type. Returns model name (for logging).\n",
    "    \"\"\"\n",
    "    raw = load_pdm_raw(data_dir, PdMConfig())\n",
    "    \n",
    "    panel = build_hourly_panel(\n",
    "        raw,\n",
    "        machine_ids=machine_ids,\n",
    "        include_model=True,\n",
    "        include_age=True,\n",
    "        one_hot_model=True,\n",
    "        drop_constant_cols=True,\n",
    "    )\n",
    "\n",
    "    col_types = infer_data_types(panel)\n",
    "    panel_scaled, scaler = scale_continuous_columns(panel, col_types)\n",
    "\n",
    "    # split into per-machine datasets and prep\n",
    "    datasets_all = {\n",
    "        mid: _prep_df(df, max_rows=max_rows_per_machine) \n",
    "        for mid, df in panel_to_machine_datasets(panel_scaled, machine_ids).items()\n",
    "    }\n",
    "    datasets_tel = {\n",
    "        mid: _prep_df(df[[c for c in TELEMETRY_COLS if c in df.columns]], max_rows=max_rows_per_machine)\n",
    "        for mid, df in datasets_all.items()\n",
    "    }\n",
    "\n",
    "    model_dir = out_dir / f\"model={model}\"\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # save a small metadata file for reproducibility\n",
    "    meta = {\n",
    "        \"model\": model,\n",
    "        \"machine_ids\": machine_ids,\n",
    "        \"tau_max\": tau_max,\n",
    "        \"sig_level\": sig_level,\n",
    "        \"max_rows_per_machine\": max_rows_per_machine,\n",
    "        \"n_vars_mixed\": len(set().union(*[set(df.columns) for df in datasets_all.values()])) if datasets_all else 0,\n",
    "        \"n_vars_telemetry\": len(set().union(*[set(df.columns) for df in datasets_tel.values()])) if datasets_tel else 0,\n",
    "        \"telemetry_cols_present\": TELEMETRY_COLS,\n",
    "        \"col_types\": {k: int(v) for k, v in col_types.items()},\n",
    "    }\n",
    "    (model_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    pcmci_per: Dict[int, Any] = {}\n",
    "    pcmci_graphs = []\n",
    "    pcmci_errors: Dict[int, str] = {}\n",
    "    \n",
    "    for mid, df_raw in panel_to_machine_datasets(panel_scaled, machine_ids).items():\n",
    "        df = _prep_df(df_raw, max_rows=max_rows_per_machine)\n",
    "        if df.shape[1] < 2:\n",
    "            pcmci_per[int(mid)] = None\n",
    "            pcmci_errors[int(mid)] = 'Too few variables after preprocessing'\n",
    "            continue\n",
    "        ct = col_types.loc[df.columns]\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            g = pcmciplus_mixed(df, tau_max=tau_max, sig_level=sig_level, col_types=ct, verbosity=0)\n",
    "            pcmci_per[int(mid)] = generalgraph_to_dict(g, metadata={'runtime_s': time.time() - t0})\n",
    "            pcmci_graphs.append(g)\n",
    "        except Exception as e:\n",
    "            pcmci_per[int(mid)] = None\n",
    "            pcmci_errors[int(mid)] = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "    pcmci_agg = None\n",
    "    pcmci_agg_meta: Dict[str, Any] = {'min_support': min_support, 'errors': pcmci_errors}\n",
    "    \n",
    "    if len(pcmci_graphs) > 0:\n",
    "        g_agg, info = aggregate_graphs(pcmci_graphs, min_support=min_support)\n",
    "        pcmci_agg = generalgraph_to_dict(g_agg, metadata={'aggregation': aggregate_info_asdict(info)})\n",
    "        pcmci_agg_meta['n_graphs_ok'] = info.n_graphs\n",
    "    else:\n",
    "        pcmci_agg_meta['n_graphs_ok'] = 0\n",
    "\n",
    "    _write_result(\n",
    "        model_dir / 'pcmci_mixed.json',\n",
    "        {\n",
    "            'algorithm': 'pcmci_plus_mixed',\n",
    "            'feature_set': 'mixed',\n",
    "            'model': model,\n",
    "            'machine_ids': machine_ids,\n",
    "            'params': {'tau_max': tau_max, 'pc_alpha': sig_level},\n",
    "            'per_machine': pcmci_per,\n",
    "            'aggregate': pcmci_agg,\n",
    "            'aggregate_meta': pcmci_agg_meta,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # ---- VARLiNGAM (per machine)\n",
    "    # Fixed: Capture tau_max in lambda default argument to avoid late binding issues\n",
    "    for feature_name, datasets in [(\"mixed\", datasets_all), (\"telemetry\", datasets_tel)]:\n",
    "        per_machine, agg_graph, agg_meta = _run_per_machine(\n",
    "            \"varlingam\",\n",
    "            lambda df, _tau=tau_max: varlingam_graph(df, tau_max=_tau, min_abs_effect=0.0),\n",
    "            datasets,\n",
    "            min_support=min_support,\n",
    "        )\n",
    "        _write_result(\n",
    "            model_dir / f\"varlingam_{feature_name}.json\",\n",
    "            {\n",
    "                \"algorithm\": \"varlingam\",\n",
    "                \"feature_set\": feature_name,\n",
    "                \"model\": model,\n",
    "                \"machine_ids\": machine_ids,\n",
    "                \"params\": {\"tau_max\": tau_max},\n",
    "                \"per_machine\": per_machine,\n",
    "                \"aggregate\": agg_graph,\n",
    "                \"aggregate_meta\": agg_meta,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # ---- GCMVL / Granger-Lasso (per machine)\n",
    "    for feature_name, datasets in [(\"mixed\", datasets_all), (\"telemetry\", datasets_tel)]:\n",
    "        per_machine, agg_graph, agg_meta = _run_per_machine(\n",
    "            \"gcmvl\",\n",
    "            lambda df, _tau=tau_max, _sig=sig_level: granger_lasso_graph(df, tau_max=_tau, sig_level=_sig, cv=5),\n",
    "            datasets,\n",
    "            min_support=min_support,\n",
    "        )\n",
    "        _write_result(\n",
    "            model_dir / f\"gcmvl_{feature_name}.json\",\n",
    "            {\n",
    "                \"algorithm\": \"gcmvl\",\n",
    "                \"feature_set\": feature_name,\n",
    "                \"model\": model,\n",
    "                \"machine_ids\": machine_ids,\n",
    "                \"params\": {\"tau_max\": tau_max, \"sig_level\": sig_level, \"cv\": 5},\n",
    "                \"per_machine\": per_machine,\n",
    "                \"aggregate\": agg_graph,\n",
    "                \"aggregate_meta\": agg_meta,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # ---- NBCB / CBNB (per machine)\n",
    "    if HYBRID_ALGORITHMS_AVAILABLE:\n",
    "        def _make_nbcbw(df, _tau=tau_max, _sig=sig_level): \n",
    "            m = NBCBw(df, _tau, _sig, linear=True, model=\"linear\", indtest=\"linear\", cond_indtest=\"linear\")\n",
    "            m.run()\n",
    "            return m.causal_graph\n",
    "\n",
    "        def _make_nbcbe(df, _tau=tau_max, _sig=sig_level): \n",
    "            m = NBCBe(df, _tau, _sig, linear=True, model=\"linear\", indtest=\"linear\", cond_indtest=\"linear\")\n",
    "            m.run()\n",
    "            return m.causal_graph\n",
    "\n",
    "        def _make_cbnbw(df, _tau=tau_max, _sig=sig_level): \n",
    "            m = CBNBw(df, _tau, _sig, linear=True, model=\"linear\", indtest=\"linear\", cond_indtest=\"linear\")\n",
    "            m.run()\n",
    "            return m.causal_graph\n",
    "\n",
    "        def _make_cbnbe(df, _tau=tau_max, _sig=sig_level): \n",
    "            m = CBNBe(df, _tau, _sig, linear=True, model=\"linear\", indtest=\"linear\", cond_indtest=\"linear\")\n",
    "            m.run()\n",
    "            return m.causal_graph\n",
    "\n",
    "        algos = [\n",
    "            (\"nbcb_w\", _make_nbcbw),\n",
    "            (\"nbcb_e\", _make_nbcbe),\n",
    "            (\"cbnb_w\", _make_cbnbw),\n",
    "            (\"cbnb_e\", _make_cbnbe),\n",
    "        ]\n",
    "\n",
    "        for algo_name, algo_fn in algos:\n",
    "            for feature_name, datasets in [(\"mixed\", datasets_all), (\"telemetry\", datasets_tel)]:\n",
    "                per_machine, agg_graph, agg_meta = _run_per_machine(\n",
    "                    algo_name,\n",
    "                    algo_fn,\n",
    "                    datasets,\n",
    "                    min_support=min_support,\n",
    "                )\n",
    "                _write_result(\n",
    "                    model_dir / f\"{algo_name}_{feature_name}.json\",\n",
    "                    {\n",
    "                        \"algorithm\": algo_name,\n",
    "                        \"feature_set\": feature_name,\n",
    "                        \"model\": model,\n",
    "                        \"machine_ids\": machine_ids,\n",
    "                        \"params\": {\"tau_max\": tau_max, \"sig_level\": sig_level},\n",
    "                        \"per_machine\": per_machine,\n",
    "                        \"aggregate\": agg_graph,\n",
    "                        \"aggregate_meta\": agg_meta,\n",
    "                    },\n",
    "                )\n",
    "    else:\n",
    "        print(f\"[WARN] Skipping NBCB/CBNB algorithms - modules not found\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6508c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"./data\")   # Folder containing PdM_*.csv files\n",
    "out_dir = Path(\"./outputs\") # Output folder\n",
    "seed = 1337                 # seed\n",
    "n_machines = 20             # Machines per model type\n",
    "models = \"\"                 # Comma-separated subset of model types (e.g., 'model1,model2'). Empty=all.\n",
    "tau_max = 12                # max time lag\n",
    "sig_level = 0.05            # threshold\n",
    "max_rows_per_machine = 0    # If >0 truncate each machine time series to this many rows\n",
    "min_support = 8             # Aggregation threshold for per-machine runs\n",
    "n_jobs = 4                  # Parallel workers over model types\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Selection computed once\n",
    "raw = load_pdm_raw(data_dir, PdMConfig())\n",
    "selection = _select_machines_by_model(raw[\"machines\"], n=n_machines, seed=seed)\n",
    "\n",
    "# Optionally filter models\n",
    "if models.strip():\n",
    "    keep = [m.strip() for m in models.split(\",\") if m.strip()]\n",
    "    selection = {m: selection[m] for m in keep if m in selection}\n",
    "\n",
    "(out_dir / \"machine_selection.json\").write_text(json.dumps(selection, indent=2))\n",
    "\n",
    "max_rows = max_rows_per_machine if max_rows_per_machine > 0 else None\n",
    "\n",
    "jobs = [(m, ids) for m, ids in selection.items()]\n",
    "\n",
    "if n_jobs <= 1:\n",
    "    for m, ids in jobs:\n",
    "        print(f\"[RUN] model={m} machines={len(ids)}\")\n",
    "        run_one_model(\n",
    "            data_dir=data_dir,\n",
    "            out_dir=out_dir,\n",
    "            model=m,\n",
    "            machine_ids=ids,\n",
    "            tau_max=tau_max,\n",
    "            sig_level=sig_level,\n",
    "            max_rows_per_machine=max_rows,\n",
    "            min_support=min_support,\n",
    "        )\n",
    "else:\n",
    "    # ProcessPoolExecutor parallelism over model types\n",
    "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "    futures = []\n",
    "    with ProcessPoolExecutor(max_workers=n_jobs) as ex:\n",
    "        for m, ids in jobs:\n",
    "            futures.append(ex.submit(\n",
    "                run_one_model,\n",
    "                data_dir=data_dir,\n",
    "                out_dir=out_dir,\n",
    "                model=m,\n",
    "                machine_ids=ids,\n",
    "                tau_max=tau_max,\n",
    "                sig_level=sig_level,\n",
    "                max_rows_per_machine=max_rows,\n",
    "                min_support=min_support,\n",
    "            ))\n",
    "        for fut in as_completed(futures):\n",
    "            try:\n",
    "                m = fut.result()\n",
    "                print(f\"[DONE] model={m}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {type(e).__name__}: {e}\")\n",
    "\n",
    "print(f\"All done. Outputs in: {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b01b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from graphs import dict_to_generalgraph\n",
    "from graph_utils import generalgraph_to_dot\n",
    "\n",
    "def render_all_graphs(outputs_dir: str, *, include_per_machine: bool = False, fmt: str = \"svg\"):\n",
    "    outputs_dir = Path(outputs_dir)\n",
    "    viz_dir = outputs_dir / \"viz\"\n",
    "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # optional: use python-graphviz rendering if available\n",
    "    try:\n",
    "        from graphviz import Source\n",
    "        has_graphviz = True\n",
    "    except Exception:\n",
    "        has_graphviz = False\n",
    "\n",
    "    def render_one(gdict, out_stem: Path, title: str):\n",
    "        g = dict_to_generalgraph(gdict)\n",
    "        dot = generalgraph_to_dot(g, title=title)\n",
    "\n",
    "        # always save DOT (portable)\n",
    "        (out_stem.with_suffix(\".dot\")).write_text(dot, encoding=\"utf-8\")\n",
    "\n",
    "        # optionally render an image if graphviz is installed\n",
    "        if has_graphviz:\n",
    "            Source(dot).render(str(out_stem), format=fmt, cleanup=True)\n",
    "\n",
    "    skip_names = {\"meta.json\", \"machine_selection.json\"}\n",
    "\n",
    "    for p in sorted(outputs_dir.rglob(\"*.json\")):\n",
    "        if p.name in skip_names or p.parts[-2:] == (\"viz\", p.name):\n",
    "            continue\n",
    "\n",
    "        d = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        rel = p.relative_to(outputs_dir).as_posix()\n",
    "\n",
    "        # Case 1: result file (has aggregate/per_machine)\n",
    "        if isinstance(d, dict) and (\"aggregate\" in d or \"per_machine\" in d):\n",
    "            if d.get(\"aggregate\"):\n",
    "                out_stem = viz_dir / (p.stem + \"_aggregate\")\n",
    "                render_one(d[\"aggregate\"], out_stem, title=f\"{rel} [aggregate]\")\n",
    "\n",
    "            if include_per_machine and isinstance(d.get(\"per_machine\"), dict):\n",
    "                for mid, gdict in d[\"per_machine\"].items():\n",
    "                    if not gdict:\n",
    "                        continue\n",
    "                    out_stem = viz_dir / f\"{p.stem}_machine_{mid}\"\n",
    "                    render_one(gdict, out_stem, title=f\"{rel} [machine {mid}]\")\n",
    "\n",
    "        # Case 2: “pure graph” json (nodes/edges at top)\n",
    "        elif isinstance(d, dict) and \"nodes\" in d and \"edges\" in d:\n",
    "            out_stem = viz_dir / (p.stem + \"_graph\")\n",
    "            render_one(d, out_stem, title=f\"{rel} [graph]\")\n",
    "\n",
    "    print(f\"Done. Wrote DOT files (and {fmt.upper()} if graphviz available) to: {viz_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "794c8fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote DOT files (and SVG if graphviz available) to: outputs/viz\n"
     ]
    }
   ],
   "source": [
    "render_all_graphs(out_dir, include_per_machine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c26ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
